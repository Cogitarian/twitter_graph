source = "google") # "osm", "stamen"
mapa_tlo.slaboszow <- ggmap(tlo_mapy.slaboszow)
#print(mapa_tlo.slaboszow)
mapka <- mapa_tlo.slaboszow +
geom_polygon(data=gminy.slaboszow,
aes(x=long, y=lat, group=group, fill=nazwa),
color='red',
alpha = 0.3) +
xlab("E") + ylab("N")
print(mapka)
# albo po początku kodu TERYT (rysowanie gmin w powiecie)
TERTY_beg <- "0222"
gminy.slaboszow <- filter(jednostki_ewidencyjne, substr(jednostki_ewidencyjne$kod_TERYT,1,nchar(TERTY_beg))== TERTY_beg)
geo.lat <- (max(gminy.slaboszow$lat)+min(gminy.slaboszow$lat))/2
geo.long <- (max(gminy.slaboszow$long)+min(gminy.slaboszow$long))/2
CenterOfMap.slaboszow <- geocode(paste(geo.lat,",",geo.long, sep="")) # latitude, longitutde
tlo_mapy.slaboszow <- get_map(c(lon=CenterOfMap.slaboszow$lon, lat=CenterOfMap.slaboszow$lat),
zoom = 9,
language = "pl",
color = "color", # "bw"
maptype = "roadmap", # "terrain", "satellite", "roadmap", "hybrid"
source = "google") # "osm", "stamen"
mapa_tlo.slaboszow <- ggmap(tlo_mapy.slaboszow)
#print(mapa_tlo.slaboszow)
mapka <- mapa_tlo.slaboszow +
geom_polygon(data=gminy.slaboszow,
aes(x=long, y=lat, group=group, fill=nazwa),
color='red',
alpha = 0.3) +
xlab("E") + ylab("N")
print(mapka)
install.packages(“profvis”, repos=”cloud.r-project.org”)
install.packages("profvis", repos=”cloud.r-project.org”)
install.packages("profvis", repos="cloud.r-project.org")
install.packages("profvis", repos="cloud.r-project.org")
library(data.table)
library(scales)
library(ggplot2)
link <- "https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/fin_data.csv"
dt <- data.table(read.csv(link))
dt[, date := as.Date(date)]
# create indexed values
dt[, idx_price := price/price[1], by = ticker]
# plot the indexed values
ggplot(dt, aes(x = date, y = idx_price, color = ticker)) +
geom_line() +
# Miscellaneous Formatting
theme_bw() + ggtitle("Price Developments") +
xlab("Date") + ylab("Pricen(Indexed 2000 = 1)") +
scale_color_discrete(name = "Company")
install.packages("data.table")
library(data.table)
library(scales)
library(ggplot2)
link <- "https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/fin_data.csv"
dt <- data.table(read.csv(link))
dt[, date := as.Date(date)]
# create indexed values
dt[, idx_price := price/price[1], by = ticker]
# plot the indexed values
ggplot(dt, aes(x = date, y = idx_price, color = ticker)) +
geom_line() +
# Miscellaneous Formatting
theme_bw() + ggtitle("Price Developments") +
xlab("Date") + ylab("Pricen(Indexed 2000 = 1)") +
scale_color_discrete(name = "Company")
# calculate the arithmetic returns
dt[, ret := price / shift(price, 1) - 1, by = ticker]
# summary table
# take only non-na values
tab <- dt[!is.na(ret), .(ticker, ret)]
# calculate the expected returns (historical mean of returns) and volatility (standard deviation of returns)
tab <- tab[, .(er = round(mean(ret), 4),
sd = round(sd(ret), 4)),
by = "ticker"]
ggplot(tab, aes(x = sd, y = er, color = ticker)) +
geom_point(size = 5) +
# Miscellaneous Formatting
theme_bw() + ggtitle("Risk-Return Tradeoff") +
xlab("Volatility") + ylab("Expected Returns") +
scale_y_continuous(label = percent, limits = c(0, 0.03)) +
scale_x_continuous(label = percent, limits = c(0, 0.1))
# load the data
link <- "https://raw.githubusercontent.com/DavZim/Efficient_Frontier/master/data/mult_assets.csv"
df <- data.table(read.csv(link))
# calculate the necessary values:
# I) expected returns for the two assets
er_x <- mean(df$x)
er_y <- mean(df$y)
# II) risk (standard deviation) as a risk measure
sd_x <- sd(df$x)
sd_y <- sd(df$y)
# III) covariance
cov_xy <- cov(df$x, df$y)
# create 1000 portfolio weights (omegas)
x_weights <- seq(from = 0, to = 1, length.out = 1000)
# create a data.table that contains the weights for the two assets
two_assets <- data.table(wx = x_weights,
wy = 1 - x_weights)
# calculate the expected returns and standard deviations for the 1000 possible portfolios
two_assets[, ':=' (er_p = wx * er_x + wy * er_y,
sd_p = sqrt(wx^2 * sd_x^2 +
wy^2 * sd_y^2 +
2 * wx * (1 - wx) * cov_xy))]
two_assets
# lastly plot the values
ggplot() +
geom_point(data = two_assets, aes(x = sd_p, y = er_p, color = wx)) +
geom_point(data = data.table(sd = c(sd_x, sd_y), mean = c(er_x, er_y)),
aes(x = sd, y = mean), color = "red", size = 3, shape = 18) +
# Miscellaneous Formatting
theme_bw() + ggtitle("Possible Portfolios with Two Risky Assets") +
xlab("Volatility") + ylab("Expected Returns") +
scale_y_continuous(label = percent, limits = c(0, max(two_assets$er_p) * 1.2)) +
scale_x_continuous(label = percent, limits = c(0, max(two_assets$sd_p) * 1.2)) +
scale_color_continuous(name = expression(omega[x]), labels = percent)
x <- rnorm(0,500)
plot(x)
x
?rnorm
x <- rnorm(5000, 0, 10)
x
plot(x)
hist(x)
x <- rnorm(10000,0,1)
plot(x)
x <- rnorm(10000,0,1)
hist(x)
profvis::profvis({x <- rnorm(10000,0,1)
hist(x)})
library(ggplot2)
library(scales)
library(MASS)
library(boot)
library(caret)
library(dplyr)
library(tidyr)
library(directlabels)
install.packages("directlabels")
install.packages("caret")
#===================setup=======================
library(ggplot2)
library(scales)
library(MASS)
library(boot)
library(caret)
library(dplyr)
library(tidyr)
library(directlabels)
set.seed(123)
# install nzelect package that has census data
devtools::install_github("ellisp/nzelect/pkg")
library(nzelect)
# drop the columns with areas' code and name
au <- AreaUnits2013 %>%
select(-AU2014, -Area_Code_and_Description)
# give meaningful rownames, helpful for some diagnostic plots later
row.names(au) <- AreaUnits2013$Area_Code_and_Description
# remove some repetition from the variable names
names(au) <- gsub("2013", "", names(au))
# restrict to areas with no missing data. If this was any more complicated (eg
# imputation),it would need to be part of the validation resampling too; but
# just dropping them all at the beginning doesn't need to be resampled; the only
# implication would be sample size which would be small impact and complicating.
au <- au[complete.cases(au), ]
#==================functions for two of the modelling strategies=====================
# The stepwise variable selection:
model_process_step <- function(the_data){
model_full <- lm(MedianIncome ~ ., data = the_data)
model_final <- stepAIC(model_full, direction = "both", trace = 0)
return(model_final)
}
# The dropping of highly collinear variables, based on Variance Inflation Factor:
model_process_vif <- function(the_data){
# remove the collinear variables based on vif
x <- 20
while(max(x) > 10){
mod1 <- lm(MedianIncome ~ . , data = the_data)
x <- sort(car::vif(mod1) , decreasing = TRUE)
the_data <- the_data[ , names(the_data) != names(x)[1]]
# message(paste("dropping", names(x)[1]))
}
model_vif <- lm(MedianIncome ~ ., data = the_data)
return(model_vif)
}
# The third strategy, full model, is only a one-liner with standard functions
# so I don't need to define a function separately for it.
#==================Different validation methods=================
#------------------simple bootstrap comparison-------------------------
# create a function suitable for boot that will return the goodness of fit
# statistics testing models against the full original sample.
compare <- function(orig_data, i){
# create the resampled data
train_data <- orig_data[i, ]
test_data <- orig_data # ie the full original sample
# fit the three modelling processes
model_step <- model_process_step(train_data)
model_vif  <- model_process_vif(train_data)
model_full <- lm(MedianIncome ~ ., data = train_data)
# predict the values on the original, unresampled data
predict_step <- predict(model_step, newdata = test_data)
predict_vif  <- predict(model_vif, newdata = test_data)
predict_full  <- predict(model_full, newdata = test_data)
# return a vector of 6 summary results
results <- c(
step_R2 = R2(predict_step, test_data$MedianIncome),
vif_R2  = R2(predict_vif, test_data$MedianIncome),
full_R2  = R2(predict_full, test_data$MedianIncome),
step_RMSE = RMSE(predict_step, test_data$MedianIncome),
vif_RMSE  = RMSE(predict_vif, test_data$MedianIncome),
full_RMSE  = RMSE(predict_full, test_data$MedianIncome)
)
return(results)
}
# perform bootstrap
Repeats <- 100
res <- boot(au, statistic = compare, R = Repeats)
# restructure results for a graphic showing root mean square error, and for
# later combination with the other results. I chose just to focus on RMSE;
# the messages are similar if R squared is used.
RMSE_res <- as.data.frame(res$t[ , 4:6])
names(RMSE_res) <- c("AIC stepwise selection", "Remove collinear variables", "Use all variables")
RMSE_res %>%
mutate(trial = 1:Repeats) %>%
gather(variable, value, -trial) %>%
# re-order levels:
mutate(variable = factor(variable, levels = c(
"Remove collinear variables", "AIC stepwise selection", "Use all variables"
))) %>%
ggplot(aes(x = trial, y = value, colour = variable)) +
geom_line() +
geom_point() +
ggtitle("'Simple' bootstrap of model fit of three different regression strategies",
subtitle = "Predicting areas' median income based on census variables") +
labs(x = "Resample id (there no meaning in the order of resamples)\n",
y = "Root Mean Square Error (higher is worse)\n",
colour = "Strategy",
caption = "Data from New Zealand Census 2013")
# store the three "simple bootstrap" RMSE results for later
simple <- apply(RMSE_res, 2, mean)
#-----------------------enhanced (optimism) bootstrap comparison-------------------
# for convenience, estimate the models on the original sample of data
orig_step <- model_process_step(au)
orig_vif <- model_process_vif(au)
orig_full <- lm(MedianIncome ~ ., data = au)
# create a function suitable for boot that will return the optimism estimates for
# statistics testing models against the full original sample.
compare_opt <- function(orig_data, i){
# create the resampled data
train_data <- orig_data[i, ]
# fit the three modelling processes
model_step <- model_process_step(train_data)
model_vif  <- model_process_vif(train_data)
model_full <- lm(MedianIncome ~ ., data = train_data)
# predict the values on the original, unresampled data
predict_step <- predict(model_step, newdata = orig_data)
predict_vif  <- predict(model_vif, newdata = orig_data)
predict_full  <- predict(model_full, newdata = orig_data)
# return a vector of 6 summary optimism results
results <- c(
step_R2 = R2(fitted(model_step), train_data$MedianIncome) - R2(predict_step, orig_data$MedianIncome),
vif_R2  = R2(fitted(model_vif), train_data$MedianIncome) - R2(predict_vif, orig_data$MedianIncome),
full_R2  = R2(fitted(model_full), train_data$MedianIncome) - R2(predict_full, orig_data$MedianIncome),
step_RMSE = RMSE(fitted(model_step), train_data$MedianIncome) - RMSE(predict_step, orig_data$MedianIncome),
vif_RMSE  = RMSE(fitted(model_vif), train_data$MedianIncome) - RMSE(predict_vif, orig_data$MedianIncome),
full_RMSE  = RMSE(fitted(model_full), train_data$MedianIncome) - RMSE(predict_full, orig_data$MedianIncome)
)
return(results)
}
# perform bootstrap
res_opt <- boot(au, statistic = compare_opt, R = Repeats)
# calculate and store the results for later
original <- c(
RMSE(fitted(orig_step), au$MedianIncome),
RMSE(fitted(orig_vif), au$MedianIncome),
RMSE(fitted(orig_full), au$MedianIncome)
)
optimism <- apply(res_opt$t[ , 4:6], 2, mean)
enhanced <- original - optimism
#------------------repeated cross-validation------------------
# The number of cross validation repeats is the number of bootstrap repeats / 10:
cv_repeat_num <- Repeats / 10
# use caret::train for the two standard models:
the_control <- trainControl(method = "repeatedcv", number = 10, repeats = cv_repeat_num)
cv_full <- train(MedianIncome ~ ., data = au, method = "lm", trControl = the_control)
cv_step <- train(MedianIncome ~ ., data = au, method = "lmStepAIC", trControl = the_control, trace = 0)
# do it by hand for the VIF model:
results <- numeric(10 * cv_repeat_num)
for(j in 0:(cv_repeat_num - 1)){
cv_group <- sample(1:10, nrow(au), replace = TRUE)
for(i in 1:10){
train_data <- au[cv_group != i, ]
test_data <- au[cv_group == i, ]
results[j * 10 + i] <- RMSE(
predict(model_process_vif(train_data), newdata = test_data),
test_data$MedianIncome)
}
}
cv_vif <- mean(results)
cv_vif_results <- data.frame(
results = results,
trial = rep(1:10, cv_repeat_num),
cv_repeat = rep(1:cv_repeat_num, each = 10)
)
#===============reporting results===============
# combine the three cross-validation results together and combined with
# the bootstrap results from earlier
summary_results <- data.frame(rbind(
simple,
enhanced,
c(mean(cv_step$resample$RMSE),
cv_vif,
mean(cv_full$resample$RMSE)
)
), check.names = FALSE) %>%
mutate(method = c("Simple bootstrap", "Enhanced bootstrap",
paste(cv_repeat_num, "repeats 10-fold\ncross-validation"))) %>%
gather(variable, value, -method)
# Draw a plot summarising the results
direct.label(
summary_results %>%
mutate(variable = factor(variable, levels = c(
"Use all variables", "AIC stepwise selection", "Remove collinear variables"
))) %>%
ggplot(aes(y = method, x = value, colour = variable)) +
geom_point(size = 3) +
labs(x = "Estimated Root Mean Square Error (higher is worse)\n",
colour = "Modelling\nstrategy",
y = "Method of estimating model fit\n",
caption = "Data from New Zealand Census 2013") +
ggtitle("Three different validation methods of three different regression strategies",
subtitle = "Predicting areas' median income based on census variables")
)
# Draw a plot summarising the results
direct.label(
summary_results %>%
mutate(variable = factor(variable, levels = c(
"Use all variables", "AIC stepwise selection", "Remove collinear variables"
))) %>%
ggplot(aes(y = method, x = value, colour = variable)) +
geom_point(size = 3) +
labs(x = "Estimated Root Mean Square Error (higher is worse)\n",
colour = "Modelling\nstrategy",
y = "Method of estimating model fit\n",
caption = "Data from New Zealand Census 2013")
#+
#	ggtitle("Three different validation methods of three different regression strategies",
#			  subtitle = "Predicting areas' median income based on census variables")
)
r <- getOption("repos")
r
r["CRAN"]
installed.packages()
lista <- installed.packages()
lista
write.csv(lista, file="packs.csv")
q()
ratio<-teams[1]/(teams[1]+teams[2])
strength<-1:2
strength[1]<-ratio*mean_total_score
strength[2]<-(1-ratio)*mean_total_score
score<-array(dim=2)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,5)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
rpois(1,2)
install.packages("quantreg")
install.packages("ggplot2")
devtools::install_github("dgrtwo/gganimate")
install.packages("gapminder")
library(gapminder)
library(ggplot2)
theme_set(theme_bw())
p <- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) +
geom_point() +
scale_x_log10()
library(gganimate)
gg_animate(p)
gg_animate(p)
gg_animate(p)
gg_animate(p)
library(twitteR)
library(ggplot2)
library(dplyr)
library(reshape2)
# dost?p do API
consumer_key <- "A3X5BcazhcNn55B2UdIQSvVSr"
consumer_secret <- "B0FwSEl8LkiKpemsePUzdvshwssT4LBA9apCzzCtpcDGY7XZ9L"
access_token <- "66435928-8G1xIXbsrOd06lrCJ7NdBvZN4bAxPM4MHatUlraPo"
access_secret <- "dUnRBUUab2aZIu80DnM4OcyFBR357ZnbWUt6xAJuHCgWW"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
library(twitteR)
library(ggplot2)
library(dplyr)
library(reshape2)
# dost?p do API
consumer_key <- "A3X5BcazhcNn55B2UdIQSvVSr"
consumer_secret <- "B0FwSEl8LkiKpemsePUzdvshwssT4LBA9apCzzCtpcDGY7XZ9L"
access_token <- "66435928-8G1xIXbsrOd06lrCJ7NdBvZN4bAxPM4MHatUlraPo"
access_secret <- "dUnRBUUab2aZIu80DnM4OcyFBR357ZnbWUt6xAJuHCgWW"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
# pobierz obserwuj?cych
getFollowers <- function(screenname)
{
me <- getUser(screenname)
# pobieranie listy followers?w
me.followers <- me$getFollowers()
me.followers.df <- twListToDF(me.followers) %>%
select(screenName, name, created, statusesCount, friendsCount, followersCount, profileImageUrl, lang, location, description)
return(me.followers.df)
}
# pobierz obserwowanych
getFriends <- function(screenname)
{
me <- getUser(screenname)
# pobieranie listy friends?w
me.friends <- me$getFriends()
me.friends.df <- twListToDF(me.friends) %>%
select(screenName, name, created, statusesCount, friendsCount, followersCount, profileImageUrl, lang, location, description)
return(me.friends.df)
}
# obserwowani przez obserwowanych przeze mnie
twuser <- "lemur78"
me.friends.df <- getFriends(twuser)
me.friends.df2 <- me.friends.df %>% filter(friendsCount != 0) #%>%
# tylko ci co maj? wi?cej ni? 100 friends?w - ?eby by?o szybciej ;)
filter(friendsCount > 10)
friends.names <- me.friends.df2$screenName
l <- data.frame(user=as.character(), friend=as.character(), value=as.integer())
l <- rbind(l, data.frame(user=twuser, friend=friends.names, value=1))
for(i in 1:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
}
l$user <- as.character(l$user)
l$friend <- as.character(l$friend)
for(i in 3:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
}
for(i in 15:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
}
for(i in 30:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
}
setwd("C:/Users/Lemur/SkyDrive/dane/RStudio_projects/twitter")
write(l, "saved_l.Rdata")
write.csv2(l, "saved_l.Rdata", sep=";")
write.csv2(l, "saved_l.csv")
for(i in 30:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
write.csv2(l, "saved_l.csv")
}
for(i in 30:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
write.csv2(l, "saved_l.csv")
}
for(i in 45:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
write.csv2(l, "saved_l.csv")
}
for(i in 45:length(friends.names))
{
print(paste(i, "user:", friends.names[i]))
f <- getFriends(friends.names[i])$screenName
l <- rbind(l, data.frame(user=friends.names[i], friend=f, value=1))
print(paste(" - following", length(f), "users"))
write.csv2(l, "saved_l.csv")
}
